{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c24918d2-9bea-4f41-8e40-f251068a65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238e27ef-c712-4d32-be7d-7f9097aab2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Abhay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abhay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Abhay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4277e6-81ce-4f13-b95a-3def98511041",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The quick brown fox jumps over the lazy dog!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d178d136-6a54-4b2f-ba69-c42e996f797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4e6c99-fc74-4e77-a9c8-4a5a5af7ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f511f028-43be-4731-a9be-88e6a1e725b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_punc = [word for word in tokens1 if word not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28038a0-0d96-43b0-93b6-9c2db3fa8fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d99654-daba-4720-bcd4-c3e3a4b2de0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170f9221-bf0a-41c4-a9cd-610afa181e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_words = [word for word in tokens_punc if word not in stopwords.words('english')]\n",
    "tokens_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abe05dbd-9d00-4ad1-b333-23468a40970c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe31fc-dea1-4cdd-86aa-f775a9049326",
   "metadata": {},
   "source": [
    "- Tokenization --> Spliting a pharse,sentence,paragarph or entire text into smaller unit(tokens) such as words is called as tokenization.\n",
    "- Stemming --> Reducing the word to there stem eg. studies get converted into studi.\n",
    "- Lemmatizer --> eg. studies it will get converted into study.\n",
    "- Difference Between --> Stemming - 1. Stemming is faster\n",
    "                                    2. It is rule based apporach\n",
    "                                    3. Accurancy is less\n",
    "                                    4. It is used for spam detection\n",
    "                         Lemmatizer - 1.It is lower\n",
    "                                      2.It is dictonary based apporach\n",
    "                                      3.Accurancy is more\n",
    "                                      4.It is used for text summerization,Chat Bots \n",
    "- Porter Stemmer --> it is used for stemming (it has got less error rate as compared to other stemmer, it produces best output as compared to other stemmer, limitation --> the stem that are produced are not always the real words, It works in 5 steps and 60 rules so it is time consuming)\n",
    "- lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff25efc-655b-41c9-af56-cd744f3eae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(word) for word in tokens_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de03b543-1f2f-4376-82b2-e8f80e1e2d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ab7302-ffd1-4160-867c-42a27392e7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_word = [stemmer.stem(word) for word in tokens_words]\n",
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db4a646-7139-4ae0-9053-c72f5674dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowstem = SnowballStemmer(language='english')\n",
    "snowstem_word = [snowstem.stem(word) for word in tokens_words]\n",
    "print(snowstem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ad62b55-7474-4d8a-b526-bc3069f5f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Abhay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng') # supervised machine learning mdoel for POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2eff3aa-687f-45cc-9c8e-3ef4479999d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e60200a3-0690-4690-a910-9284e8416074",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69013cd7-7fcf-485a-8b83-6e2f81dc32ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'JJ'),\n",
       " ('jump', 'NN'),\n",
       " ('lazy', 'NN'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be669f76-a532-42d0-b16d-88d0da3f9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n-grams\n",
    "bigrams = list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa68f3bd-6d5c-45ce-b76f-0dbe448d538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('quick', 'brown'), ('brown', 'fox'), ('fox', 'jump'), ('jump', 'lazy'), ('lazy', 'dog')]\n"
     ]
    }
   ],
   "source": [
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "433fb5fe-e8e6-44fd-8ee6-27dc44c5bb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
      "Lowercased: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
      "Without Punctuation: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
      "Without Stopwords: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
      "Lemmatized: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
      "Stemmed Tokens: ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
      "POS TAGS: [('quick', 'JJ'), ('brown', 'NN'), ('fox', 'JJ'), ('jump', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n",
      "Bigrams: [('quick', 'brown'), ('brown', 'fox'), ('fox', 'jump'), ('jump', 'lazy'), ('lazy', 'dog')]\n"
     ]
    }
   ],
   "source": [
    "# Text \n",
    "tokens1 = word_tokenize(text)\n",
    "print('Tokens:',tokens)\n",
    "\n",
    "# Lowerased\n",
    "tokens1 = [word.lower() for word in tokens]\n",
    "print('Lowercased:',tokens1)\n",
    "\n",
    "#3 Remove Pucnctuations\n",
    "tokens1 = [word for word in tokens1 if word not in string.punctuation]\n",
    "print('Without Punctuation:',tokens1)\n",
    "\n",
    "#4 Remove stopwords\n",
    "stop_word = set(stopwords.words('english'))\n",
    "tokens1 = [word for word in tokens1 if word not in stop_word]\n",
    "print('Without Stopwords:',tokens1)\n",
    "\n",
    "#5 Lemmatization\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatized_token = [lemmatizer.lemmatize(word) for word in tokens1]\n",
    "print('Lemmatized:',lemmatized_token)\n",
    "\n",
    "# 6 Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in lemmatized_token]\n",
    "print('Stemmed Tokens:', stemmed_tokens)\n",
    "\n",
    "# 7 POS Tagging\n",
    "pos_tags = pos_tag(tokens1)\n",
    "print('POS TAGS:',pos_tags)\n",
    "\n",
    "# 8 Generate N-Grams\n",
    "bigrams = list(ngrams(tokens1,2))\n",
    "print('Bigrams:',bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf98c5-06d4-4b57-8d72-72fa43fb3a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae5615e7-35d5-4467-9efc-aa56fe2cc86b",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84617532-3329-4f2a-9a1d-32e60a449be0",
   "metadata": {},
   "source": [
    "- It is a text representation technique where the text is represented as collection (Bag)\n",
    "- Collection of words where the grammar and the order of the word is not taken in count but frequency matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06050b3c-13d5-439d-a8c9-0d4ae9c83ef5",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "2. Vocabulary\n",
    "3. Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09c4a9ad-31bc-4e6d-bfde-7130acf976da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n",
      "[[0 1 1 1]\n",
      " [1 1 0 1]]\n",
      "['fun' 'learning' 'love' 'machine']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = ['I Love machine learning', 'Machine learning is fun']\n",
    "\n",
    "# initialize countviser\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "#fit and transform the documents into a bag of words\n",
    "x = vectorizer.fit_transform(documents)\n",
    "print(x)\n",
    "\n",
    "# convert it into an array\n",
    "print(x.toarray())\n",
    "\n",
    "# display the vocabulary (features)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25aaae-1554-47a4-9614-3e33247b3bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cb8ca19-ed83-4181-b120-482d504dccfe",
   "metadata": {},
   "source": [
    "## Limitations of Bag of Words\n",
    "\n",
    "- Loss of context --> \n",
    "- High Dimensionality / Vector Size\n",
    "- There is no semantic meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478ff0a-af08-4551-b47b-5e549e24a0cb",
   "metadata": {},
   "source": [
    "#### TF - Term Frequency\n",
    "#### IDF - Inverse Document Frequency\n",
    "\n",
    "\n",
    "- Statistical method to evaluate the importance of the word in the document by calculating the score of each of the word.\n",
    "- Generally used for information retrieval and summarization.\n",
    "  \n",
    "- **TF** = no of repetation of words in a sentence / total no of words in a sentence ----> \n",
    "- **IDF** = No of sentences in numerator / No of sentences containing the word ---> How unique the word is in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89278a59-d120-4595-8f8a-4aabb0060656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['at' 'barked' 'cat' 'dog' 'mat' 'on' 'sat' 'the']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.30253071 0.         0.42519636 0.42519636\n",
      "  0.42519636 0.60506143]\n",
      " [0.42519636 0.42519636 0.30253071 0.42519636 0.         0.\n",
      "  0.         0.60506143]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"The Cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\"\n",
    "]\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "x = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"Vocabulary: \", vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\\n\", x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06600092-7ea3-4d76-8731-c4b34bce759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2f4d121-e3bf-48a4-a6ef-5d725d27c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', ' banana', ' orange']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r\",\", \"apple, banana, orange\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5f733e9-c987-4d19-b63c-b8aef7997260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['123', '456']\n",
      "['Hello', 'hello']\n",
      "Pattern found!\n",
      "re.compile('^abc')\n",
      "Hello\\.\\ How\\ are\\ you\\?\\ \\[123\\]\n",
      "Pattern found!\n",
      "['123', '456']\n",
      "[('John', '25'), ('Mike', '30'), ('Alice', '22')]\n",
      "['Python', 'PYTHON']\n",
      "<re.Match object; span=(0, 5), match='Hello'>\n",
      "None\n",
      "('123', '45', '6789')\n",
      "None\n",
      "<re.Match object; span=(6, 11), match='world'>\n",
      "<re.Match object; span=(0, 5), match='12345'>\n",
      "None\n",
      "<re.Match object; span=(0, 5), match='Hello'>\n",
      "<re.Match object; span=(0, 17), match='file_name-123.txt'>\n",
      "None\n",
      "<re.Match object; span=(0, 3), match='123'>\n",
      "None\n",
      "['This', 'is', 'a', 'test', 'string']\n",
      "['apple', 'banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"Argmax returns the index of the maximum value of probability by ignoring rest of the probability distribution which might be useful.\"\"\"\n",
    "\n",
    "probabilities=[0.1,0.3,0.56,0.8,0.47]\n",
    "predicted_class=np.argmax(probabilities)\n",
    "print(predicted_class)\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Compile the regular expression\n",
    "pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "# Use the compiled object to match text\n",
    "result = pattern.findall(\"There are 123 apples and 456 oranges.\")\n",
    "print(result)\n",
    "\n",
    "# Compile the pattern with a flag (case-insensitive match)\n",
    "import re\n",
    "pattern = re.compile(r\"hello\", re.IGNORECASE)\n",
    "\n",
    "# Match using the compiled pattern\n",
    "result = pattern.findall(\"Hello world! hello again!\")\n",
    "print(result)\n",
    "\n",
    "\"\"\"re.search(specified pattern exists anywhere in the string)\"\"\"\n",
    "\n",
    "pattern = re.compile(\"^abc\",re.IGNORECASE)\n",
    "re.search(pattern, \"Abcdef\") #match not found returns none\n",
    "re.search(\"xyz$\", \"hello world xyz\")#match found\n",
    "\n",
    "# Define the pattern\n",
    "pattern = r\"apple\"\n",
    "\n",
    "# Check if the pattern is in the string\n",
    "text = \"I have an apple and an orange.\"\n",
    "if re.search(pattern, text):\n",
    "    print(\"Pattern found!\")\n",
    "else:\n",
    "    print(\"Pattern not found.\")\n",
    "\n",
    "pattern = re.compile(\"^abc\")\n",
    "print(pattern)\n",
    "pattern = re.compile(\"end$\")\n",
    "pattern = re.compile(\"a.b\")\n",
    "\n",
    "\"\"\"re.escape(escape all non-alphanumeric characters in a string)\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "# Original string with special characters\n",
    "special_string = \"Hello. How are you? [123]\"\n",
    "\n",
    "# Escape special characters\n",
    "escaped_string = re.escape(special_string)\n",
    "\n",
    "print(escaped_string)\n",
    "# Output: Hello\\. How are you\\? \\[123\\]\n",
    "\n",
    "\"\"\"Using escaped string in regex\"\"\"\n",
    "\n",
    "# A string with special characters\n",
    "user_input = \"file.name[1]\"\n",
    "\n",
    "# Escape the string for use in regex\n",
    "escaped_input = re.escape(user_input)\n",
    "\n",
    "# Use the escaped string as a pattern\n",
    "pattern = re.compile(escaped_input)\n",
    "\n",
    "# Test the pattern\n",
    "text = \"Here is the file: file.name[1]\"\n",
    "if pattern.search(text):\n",
    "    print(\"Pattern found!\")\n",
    "else:\n",
    "    print(\"Pattern not found.\")\n",
    "\n",
    "\"\"\"re.findall(find all occurrences of a pattern in a string and returns list of strings)\"\"\"\n",
    "\n",
    "pattern = r\"\\d+\"\n",
    "result = re.findall(pattern, \"123 abc 456\")\n",
    "print(result)\n",
    "\n",
    "\"\"\"Search with groups\"\"\"\n",
    "\n",
    "text = \"John: 25, Mike: 30, Alice: 22\"\n",
    "\n",
    "# Find names and ages\n",
    "matches = re.findall(r\"(\\w+): (\\d+)\", text)\n",
    "\n",
    "print(matches)\n",
    "\n",
    "\"\"\"Case insensitive\"\"\"\n",
    "\n",
    "text = \"Python is Fun. PYTHON is powerful.\"\n",
    "\n",
    "# Case-insensitive search\n",
    "matches = re.findall(r\"python\", text, re.IGNORECASE)\n",
    "\n",
    "print(matches)\n",
    "\n",
    "\"\"\"re.match(pattern matches the beginning of a string. Returns object or None)\"\"\"\n",
    "\n",
    "#pattern for word\n",
    "pattern = r\"\\w+\"\n",
    "\n",
    "# Match from the start of the string\n",
    "result = re.match(pattern, \"Hello world!\")\n",
    "print(result)\n",
    "\n",
    "# No match at the start\n",
    "result = re.match(pattern, \"  Hello world!\")\n",
    "print(result)\n",
    "\n",
    "\"\"\"Matching in groups\"\"\"\n",
    "\n",
    "#Pattern with capturing group\n",
    "pattern = r\"(\\d{3})-(\\d{2})-(\\d{4})\"\n",
    "\n",
    "# Match a specific pattern\n",
    "result = re.match(pattern, \"123-45-6789\")\n",
    "if result:\n",
    "    print(result.groups())\n",
    "\n",
    "\"\"\"Difference between match and search\"\"\"\n",
    "\n",
    "pattern = r\"world\"\n",
    "\n",
    "# re.match\n",
    "print(re.match(pattern, \"Hello world\"))\n",
    "\n",
    "# re.search\n",
    "print(re.search(pattern, \"Hello world\"))\n",
    "\n",
    "\"\"\"re.fullmatch\"\"\"\n",
    "\n",
    "# Pattern for a number\n",
    "pattern = r\"\\d+\"\n",
    "\n",
    "# Full match\n",
    "print(re.fullmatch(pattern, \"12345\"))\n",
    "\n",
    "# Partial match\n",
    "print(re.fullmatch(pattern, \"12345abc\"))\n",
    "\n",
    "# Case-insensitive full match\n",
    "pattern = r\"hello\"\n",
    "print(re.fullmatch(pattern, \"Hello\", re.IGNORECASE))\n",
    "\n",
    "\"\"\"Using special characters\"\"\"\n",
    "\n",
    "# Pattern for a valid file name\n",
    "pattern = r\"[a-zA-Z0-9_-]+\\.txt\"\n",
    "\n",
    "print(re.fullmatch(pattern, \"file_name-123.txt\"))\n",
    "print(re.fullmatch(pattern, \"invalid-file.jpeg\"))\n",
    "\n",
    "\"\"\"match and fullmatch\"\"\"\n",
    "\n",
    "pattern = r\"\\d+\"\n",
    "\n",
    "# re.match\n",
    "print(re.match(pattern, \"123abc\"))  # Output: <re.Match object; span=(0, 3), match='123'>\n",
    "\n",
    "# re.fullmatch\n",
    "print(re.fullmatch(pattern, \"123abc\"))\n",
    "\n",
    "\"\"\"re.split()\"\"\"\n",
    "\n",
    "#Split on whitespace\n",
    "result = re.split(r\"\\s+\", \"This is a test string\")\n",
    "print(result)\n",
    "\n",
    "\"\"\"splitting on specific character\"\"\"\n",
    "\n",
    "# Split on commas\n",
    "result = re.split(r\",\", \"apple,banana,orange\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11277ba3-d588-4f26-a80f-9603ed5c76da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33d8f5b9-2d6b-4199-934e-14477280c836",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "- Are dense vector representations of words generated through neural networks.\n",
    "- Each word is mapped to a high dimensional vector space where the relationships between the words are reserved.\n",
    "\n",
    "### Adv:\n",
    "\n",
    "1. Semantic and Syntactic information are reserved.\n",
    "2. Embeddings trained on large text / large corpus understand the word relationship.\n",
    "3. Accuracy is improved.\n",
    "\n",
    "\n",
    "### Models for Word Embeddings:\n",
    "\n",
    "1. Word2Vec ---> Word is converted to vector. It is pretrained by Google\n",
    "2. C-Bow -----> Continuous Bag of Words.\n",
    "3. Skip-Gram ---->\n",
    "4. Glove -------> It is of Stanford University.\n",
    "5. Fast-Text -----> It is from Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f9069-bce8-49fb-9b1c-a05402e7116d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
